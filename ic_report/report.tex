\documentclass[11pt,a4paper]{article}
\usepackage[T1]{fontenc}
\usepackage[portuguese]{babel}
\usepackage[utf8]{inputenc}
\inputencoding{utf8}
\usepackage{url}
%\usepackage{siunitx} % Provides the \SI{}{} and \si{} command for typesetting SI units
\usepackage{graphicx} % Required for the inclusion of images
\usepackage{caption}
\usepackage{subcaption}
\usepackage{natbib} % Required to change bibliography style to APA
\usepackage{enumerate}
\usepackage{amsmath} % Required for some math elements 
\usepackage{graphicx}
\usepackage{epstopdf} %use postscript
\usepackage{float}
\usepackage{a4wide}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{wrapfig}
\usepackage{tikz}
\usepackage{multicol}
\usetikzlibrary{shapes,arrows,positioning,calc}
\usepackage{amsthm} % Math packages
%\usepackage[headheight=14pt]{geometry}
\usepackage{sectsty} % Allows customizing section commands
\usepackage{fancyhdr} % Custom headers and footers
%\usepackage[margin=1in]{geometry} % Change margin
\usepackage{longtable}

\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
%\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyhead[L]{Relatório Preliminar}% Empty left footer
\fancyhead[R]{Inteligência Computacional} % Empty center footer
\fancyfoot[C]{\thepage} 								% Page numbering for right footer
\numberwithin{equation}{section}


\begin{document}

\input{title}
\section{Introdução}
Este trabalho foi desenvolvido sobre o \textit{dataset Breast Cancer Wisconsin (Diagnostic)}, fornecido pelo  \textit{UCI Machine Learning}. As variáveis deste \textit{dataset} são características computadas a partir de imagens digitalizadas de exames de mama por punção aspirativa por agulha fina. Os dados descrevem características do núcleo das células presentes na imagem. 
\section{Caracterização}

\subsection{Dados}
%O dataset possui os seguintes dados.

%\begin{itemize}
%\item ID do paciente.
%\item 
%\end{itemize}

Inicialmente é importante ressaltar que o conjunto de dados gerado pelos exames é tridimensional. Para cada registro (paciente) existe um conjunto de células, cada uma com os seguintes atributos.

\begin{enumerate}
\item Raio (média das distâncias do centro para pontos no perímetro)
\item Textura
\item Perímetro
\item Área
\item Suavidade
\item Compacidade
\item Concavidade
\item Pontos côncavos
\item Simetria
\item Dimensão Fractal
\end{enumerate}

De forma a eliminar a terceira dimensão (do conjunto de células), para cada conjunto de células o \textit{UCI Machine Learning} forneceu o dataset com valores de média, desvio(erro) padrão e o pior valor dos atributos no universo das células, resultando em 30 variáveis de entrada. O dataset conforme obtido possui as variáveis como segue:

\begin{multicols}{3}
\begin{enumerate}
    \item radius\_mean	
    \item texture\_mean
    \item perimeter\_mean
    \item area\_mean
    \item smoothness\_mean
    \item compactness\_mean
    \item concavity\_mean
    \item concave points\_mean
    \item symmetry\_mean
    \item fractal\_dimension\_mean
    \item radius\_se
    \item texture\_se
    \item perimeter\_se
    \item area\_se
    \item smoothness\_se
    \item compactness\_se
    \item concavity\_se
    \item concave points\_se
    \item symmetry\_se
    \item fractal\_dimension\_se
    \item radius\_worst
    \item texture\_worst
    \item perimeter\_worst
    \item area\_worst
    \item smoothness\_worst
    \item compactness\_worst
    \item concavity\_worst
    \item concave points\_worst
    \item symmetry\_worst
    \item fractal\_dimension\_worst
\end{enumerate}
\end{multicols}

A variável de saída é o diagnóstico (maligno ou benigno codificados como $-1$ e $1$ respectivamente). Existe uma coluna com o ID do paciente que foi eliminada por ser irrelevante. O dataset apresenta 357 amostras benignas e 212 amostras malignas. 

%\newpage
\subsection{Estatísticas Básicas e Histogramas}

\begin{itemize}
\item Radius
\begin{figure}[H]
\centering
  \includegraphics[width=.5\linewidth]{../img/hist/radius_mean}
  \captionof{figure}{Mean}
  \label{fig:test1}
\end{figure}%

\begin{figure}[H]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{../img/hist/radius_se}
  \captionof{figure}{Standard Error}
  \label{fig:test1}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{../img/hist/radius_worst}
  \captionof{figure}{Worst}
  \label{fig:test2}
\end{minipage}
\end{figure}

\begin{table}[H]
\centering
\caption{Radius}
\label{my-label}
\begin{tabular}{lllll}
\hline
              & radius\_mean & radius\_se & radius\_worst &  \\
\hline
Máximo        & 28.11        & 2.873      & 36.04         &  \\
Mínimo        & 6.981        & 0.1115     & 7.93          &  \\
Média         & 14.12729     & 0.405172   & 16.26919      &  \\
Desvio padrão & 3.524049     & 0.277313   & 4.833242      &  \\
Percentil 25  & 11.7         & 0.2324     & 13.01         &  \\
Percentil 50  & 13.37        & 0.3242     & 14.97         &  \\
Percentil 75  & 15.78        & 0.4789     & 18.79         &  \\
\hline
\end{tabular}
\end{table}


Análise: Para a variável Radius mean, vemos que a maioria de seus valores se concentram mais proximos da média que é 14,13.
Para Radius Standard Error, também têm um coportamento semelhante a uma função de cauda longa, porém não temos a presença de valores no intervalo entre 1,6 e 2,4.
Já para a variável Radius Worst, tem um comportamento semelhante à variável Radius mean.

\item Texture
\begin{figure}[H]
\centering
  \includegraphics[width=.5\linewidth]{../img/hist/texture_mean}
  \captionof{figure}{Mean}
  \label{fig:test1}
\end{figure}%

\begin{figure}[H]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{../img/hist//texture_se}
  \captionof{figure}{Standard Error}
  \label{fig:test1}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{../img/hist/texture_worst}
  \captionof{figure}{Worst}
  \label{fig:test2}
\end{minipage}
\end{figure}

\begin{table}[H]
\centering
\caption{Texture}
\label{my-label}
\begin{tabular}{lllll}
\hline
              & texture\_mean & texture\_se & texture\_worst &  \\ \hline
Máximo        & 39.28         & 4.885       & 49.54          &  \\
Mínimo        & 9.71          & 0.3602      & 12.02          &  \\
Média         & 19.28964851   & 1.216853427 & 25.67722       &  \\
Desvio padrão & 4.301035768   & 0.551648393 & 6.146258       &  \\
Percentil 25  & 16.17         & 0.8339      & 21.08          &  \\
Percentil 50  & 18.84         & 1.108       & 25.41          &  \\
Percentil 75  & 21.8          & 1.474       & 29.72          & \\ \hline
\end{tabular}
\end{table}

Análise: Podemos perceber que a variável Texture Mean, tem um comportamento que lembra a uma função Gaussiana, que de certa forma é espelhada em realação a média, com a exceção dos outliers. Em Texture Standart Error, a média é 1,22  e seus valores estão localizados proóximos á média, porém temos uma certa quantidade de valores distantes, mesmo considerando o desvio parão, e um valor máximo muito alto. Em Texture Worst, vemos que seu comportamento se assemelha a Texture Mean.


\item Perimeter
\begin{figure}[H]
\centering
  \includegraphics[width=.5\linewidth]{../img/hist/perimeter_mean}
  \captionof{figure}{Mean}
  \label{fig:test1}
\end{figure}%

\begin{figure}[H]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{../img/hist/perimeter_se}
  \captionof{figure}{Standard Error}
  \label{fig:test1}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{../img/hist/perimeter_worst}
  \captionof{figure}{Worst}
  \label{fig:test2}
\end{minipage}
\end{figure}


\begin{table}[H]
\centering
\caption{Perimeter}
\label{my-label}
\begin{tabular}{lllll} \hline
              & perimeter\_mean & perimeter\_se & perimeter\_worst &  \\ \hline
Máximo        & 188.5           & 21.98         & 251.2            &  \\
Mínimo        & 43.79           & 0.757         & 50.41            &  \\
Média         & 91.96903339     & 2.866059227   & 107.2612         &  \\
Desvio padrão & 24.29898104     & 2.021854554   & 33.60254         &  \\
Percentil 25  & 75.17           & 1.606         & 84.11            &  \\
Percentil 50  & 86.24           & 2.287         & 97.66            &  \\
Percentil 75  & 104.1           & 3.357         & 125.4            & \\ \hline
\end{tabular}
\end{table}

Análise: Em Perimeter Standard Error, vemos a presença de outliers, como por exemplo o valor máximo que é 21,98,  enquanto sua média é 2.87. E em Perimeter Worst, vemos que possui um desvio padrão alto e seus valores estão distribuídos de forma distante da média.


\item Area
\begin{figure}[H]
\centering
  \includegraphics[width=.5\linewidth]{../img/hist/area_mean}
  \captionof{figure}{Mean}
  \label{fig:test1}
\end{figure}%

\begin{figure}[H]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{../img/hist/area_se}
  \captionof{figure}{Standard Error}
  \label{fig:test1}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{../img/hist/area_worst}
  \captionof{figure}{Worst}
  \label{fig:test2}
\end{minipage}
\end{figure}

\begin{table}[H]
\centering
\caption{Area}
\label{my-label}
\begin{tabular}{lllll} \hline
              & area\_mean  & area\_se    & area\_worst &  \\ \hline
Máximo        & 2501        & 542.2       & 4254        &  \\
Mínimo        & 143.5       & 6.802       & 185.2       &  \\
Média         & 654.8891037 & 40.33707909 & 880.5831283 &  \\
Desvio padrão & 351.9141292 & 45.49100552 & 569.3569927 &  \\
Percentil 25  & 420.3       & 17.85       & 515.3       &  \\
Percentil 50  & 551.1       & 24.53       & 686.5       &  \\
Percentil 75  & 782.7       & 45.19       & 1084        &  \\ \hline
\end{tabular}
\end{table}

Análise: Na variável Area Mean, vemos que ela possui um desvio padrão grande, sendo maior que a metade da média, assim como em Area Worst. Em Area Standard Error, vemos que a variável tem um compartamento semelhante a uma função de cauda longa e temos uma valor bem distante que é o valor máximo ( 2501,00). 


\item Smoothness
\begin{figure}[H]
\centering
  \includegraphics[width=.5\linewidth]{../img/hist/smoothness_mean}
  \captionof{figure}{Mean}
  \label{fig:test1}
\end{figure}%

\begin{figure}[H]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{../img/hist/smoothness_se}
  \captionof{figure}{Standard Error}
  \label{fig:test1}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{../img/hist/smoothness_worst}
  \captionof{figure}{Worst}
  \label{fig:test2}
\end{minipage}
\end{figure}

\begin{table}[H]
\centering
\caption{Smoothness}
\label{my-label}
\begin{tabular}{lllll} \hline
              & smoothness\_mean & smoothness\_se & smoothness\_worst &  \\ \hline
Máximo        & 0.1634           & 0.03113        & 0.2226            &  \\
Mínimo        & 0.05263          & 0.001713       & 0.07117           &  \\
Média         & 0.096360281      & 0.007040979    & 0.132368594       &  \\
Desvio padrão & 0.014064128      & 0.003002518    & 0.022832429       &  \\
Percentil 25  & 0.08637          & 0.005169       & 0.1166            &  \\
Percentil 50  & 0.09587          & 0.00638        & 0.1313            &  \\
Percentil 75  & 0.1053           & 0.008146       & 0.146             & \\ \hline
\end{tabular}
\end{table}

Análise: Podemos ver que tanto Smoothness Mean quanto em Worst, elas tem uma aparência semelhante a uma função Gaussiana e possuem um desvio padrão pequeno, já em Smoothness Standard Error, vemos que ela possui um desvio padrão alto e existe a presença de outliers como o seu valor máximo (0,16340).


\item Compactness
\begin{figure}[H]
\centering
  \includegraphics[width=.5\linewidth]{../img/hist/compactness_mean}
  \captionof{figure}{Mean}
  \label{fig:test1}
\end{figure}%


\begin{figure}[H]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{../img/hist/compactness_se}
  \captionof{figure}{Standard Error}
  \label{fig:test1}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{../img/hist/compactness_worst}
  \captionof{figure}{Worst}
  \label{fig:test2}
\end{minipage}
\end{figure}

\begin{table}[H]
\centering
\caption{Compactness}
\label{my-label}
\begin{tabular}{lllll}\hline
              & compactness\_mean & compactness\_se & compactness\_worst &  \\ \hline
Máximo        & 0.3454            & 0.1354          & 1.058              &  \\
Mínimo        & 0.01938           & 0.002252        & 0.02729            &  \\
Média         & 0.104340984       & 0.025478139     & 0.254265           &  \\
Desvio padrão & 0.052812758       & 0.017908179     & 0.157336           &  \\
Percentil 25  & 0.06492           & 0.01308         & 0.1472             &  \\
Percentil 50  & 0.09263           & 0.02045         & 0.2119             &  \\
Percentil 75  & 0.1304            & 0.03245         & 0.3391             &  \\ \hline
\end{tabular}
\end{table}

Análise: Aqui percebemos que as 3 variáveis possuem um desvio padrão alto e seus valores máximos se destoam bantante.

\item Concavity
\begin{figure}[H]
\centering
  \includegraphics[width=.5\linewidth]{../img/hist/concavity_mean}
  \captionof{figure}{Mean}
  \label{fig:test1}
\end{figure}%


\begin{figure}[H]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{../img/hist/concavity_se}
  \captionof{figure}{Standard Error}
  \label{fig:test1}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{../img/hist/concavity_worst}
  \captionof{figure}{Worst}
  \label{fig:test2}
\end{minipage}
\end{figure}

\begin{table}[H]
\centering
\caption{Concavity}
\label{my-label}
\begin{tabular}{lllll} \hline
              & concavity\_mean & concavity\_se & concavity\_worst &  \\ \hline
Máximo        & 0.4268          & 0.396         & 1.252            &  \\
Mínimo        & 0               & 0             & 0                &  \\
Média         & 0.088799316     & 0.031893716   & 0.272188483      &  \\
Desvio padrão & 0.079719809     & 0.03018606    & 0.208624281      &  \\
Percentil 25  & 0.02956         & 0.01509       & 0.1145           &  \\
Percentil 50  & 0.06154         & 0.02589       & 0.2267           &  \\
Percentil 75  & 0.1307          & 0.04205       & 0.3829           & \\ \hline
\end{tabular}
\end{table}

Análise: Nas 3 variáveis percebemos que o seus valores se concentram mais proximos de 0 e a ocorrência desses valores vão decaindo conforme se afastam de 0.

\item Concave points
\begin{figure}[H]
\centering
  \includegraphics[width=.5\linewidth]{../img/hist/concave_points_mean}
  \captionof{figure}{Mean}
  \label{fig:test1}
\end{figure}%

\begin{figure}[H]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{../img/hist/concave_points_se}
  \captionof{figure}{Standard Error}
  \label{fig:test1}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{../img/hist/concave_points_worst}
  \captionof{figure}{Worst}
  \label{fig:test2}
\end{minipage}
\end{figure}

\begin{table}[H]
\centering
\caption{Concave points}
\label{my-label}
\begin{tabular}{lllll} \hline
              & concave points\_mean & concave points\_se & concave points\_worst &  \\ \hline
Máximo        & 0.2012               & 0.05279            & 0.291                 &  \\
Mínimo        & 0                    & 0                  & 0                     &  \\
Média         & 0.048919146          & 0.011796           & 0.114606              &  \\
Desvio padrão & 0.038802845          & 0.00617            & 0.065732              &  \\
Percentil 25  & 0.02031              & 0.007638           & 0.06493               &  \\
Percentil 50  & 0.0335               & 0.01093            & 0.09993               &  \\
Percentil 75  & 0.074                & 0.01471            & 0.1614                &  \\ \hline
\end{tabular}
\end{table}

Análise: Aqui vemos que a variável Cancave points mean, tem um comportamento semelhante  à uma função de cauda longa e que a variável Cancave Points Standard Error possui alguns outliers,  como o valor máximo por exemplo.

\item Symmetry
\begin{figure}[H]
\centering
  \includegraphics[width=.5\linewidth]{../img/hist/symmetry_mean}
  \captionof{figure}{Mean}
  \label{fig:test1}
\end{figure}%

\begin{figure}[H]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{../img/hist/symmetry_se}
  \captionof{figure}{Standard Error}
  \label{fig:test1}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{../img/hist/symmetry_worst}
  \captionof{figure}{Worst}
  \label{fig:test2}
\end{minipage}
\end{figure}


\begin{table}[H]
\centering
\caption{Symmetry}
\label{my-label}
\begin{tabular}{lllll} \hline
              & symmetry\_mean & symmetry\_se & symmetry\_worst &  \\ \hline
Máximo        & 0.304          & 0.07895      & 0.6638          &  \\
Mínimo        & 0.106          & 0.007882     & 0.1565          &  \\
Média         & 0.181162       & 0.020542     & 0.290076        &  \\
Desvio padrão & 0.027414       & 0.008266     & 0.061867        &  \\
Percentil 25  & 0.1619         & 0.01516      & 0.2504          &  \\
Percentil 50  & 0.1792         & 0.01873      & 0.2822          &  \\
Percentil 75  & 0.1957         & 0.02348      & 0.3179          &  \\ \hline
\end{tabular}
\end{table}

Análise - A variável Symmetry mean possui um comportamento semelhante  a uma função Gaussiana e tanto Symmetry Standard Error, quanto Wosrt possuem valores máximos distantes da média.

\item Fractal Dimension
\begin{figure}[H]
\centering
  \includegraphics[width=.5\linewidth]{../img/hist/fractal_dimension_mean}
  \captionof{figure}{Mean}
  \label{fig:test1}
\end{figure}%

\begin{figure}[H]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{../img/hist/fractal_dimension_se}
  \captionof{figure}{Standard Error}
  \label{fig:test1}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{../img/hist/fractal_dimension_worst}
  \captionof{figure}{Worst}
  \label{fig:test2}
\end{minipage}
\end{figure}
\end{itemize}

\begin{table}[H]
\centering
\caption{Fractal dimension}
\label{my-label}
\begin{tabular}{lllll} \hline
              & fractal\_dimension\_mean & fractal\_dimension\_se & fractal\_dimension\_worst &  \\ \hline
Máximo        & 0.09744                  & 0.02984                & 0.2075                    &  \\
Mínimo        & 0.04996                  & 0.000895               & 0.05504                   &  \\
Média         & 0.06279761               & 0.003795               & 0.083945817               &  \\
Desvio padrão & 0.007060363              & 0.002646               & 0.018061267               &  \\
Percentil 25  & 0.0577                   & 0.002248               & 0.07146                   &  \\
Percentil 50  & 0.06154                  & 0.003187               & 0.08004                   &  \\
Percentil 75  & 0.06612                  & 0.004558               & 0.09208                   &  \\ \hline
\end{tabular}
\end{table}

Análise:   Podemos ver que apartir da média, a ocorrência dos valores das váreiaveis vão  diminuindo conforme se distanciam da média.

A partir dos histogramas podemos avaliar que em geral não revelou características indesejáveis como distribuições multimodais. Conforme comentado algumas apresentaram assimetria.

\subsection{Matriz de Correlação}
\begin{figure}[H]
\centering
  \includegraphics[width=\linewidth]{./img/corrcoef_range.png}
  \captionof{figure}{Matriz de Correlação}
  \label{fig:test1}
\end{figure}%

A partir da matriz podemos concluir que as seguintes variáveis estão fortemente correlacionadas (apresentam coeficiente de correlação acima de 0.9):

\begin{multicols}{2}
\begin{itemize}
	 \item Radius Mean, Perimeter Mean
	 \item Radius Mean, Area Mean
	 \item Radius Mean, Radius Worst
	 \item Radius Mean, Perimeter Worst
	 \item Radius Mean, Area Worst
	 \item Texture Mean, Texture Worst
	 \item Perimeter Mean, Area Mean
	 \item Perimeter Mean, Radius Worst
	 \item Perimeter Mean, Perimeter Worst
	 \item Perimeter Mean, Area Worst
	 \item Area Mean, Radius Worst
	 \item Area Mean, Perimeter Worst
	 \item Area Mean, Area Worst
	 \item Concavity Mean, Concave Points Mean
	 \item Concave Points Mean, Concave Points Worst
	 \item Radius SE, Perimeter SE
	 \item Radius SE, Area SE
	 \item Perimeter SE, Area SE
 	 \item Radius Worst, Perimeter Worst
	 \item Radius Worst, Area Worst
	 \item Perimeter Worst, Area Worst
\end{itemize}
\end{multicols}
As seguintes variáveis apresentaram correlação negativa:

\begin{multicols}{2}
\begin{itemize}
\item Radius Mean, Fractal Dimension Mean
\item Radius Mean, Texture SE
\item Radius Mean, Smoothness SE
\item Radius Mean, Symmetry SE
\item Radius Mean, Fractal Dimension SE
\item Texture Mean, Smoothness Mean
\item Texture Mean, Fractal Dimension Mean
\item Perimeter Mean, Fractal Dimension Mean
\item Perimeter Mean, Texture SE
\item Perimeter Mean, Smoothness SE
\item Perimeter Mean, Symmetry SE
\item Perimeter Mean, Fractal Dimension SE
\item Area Mean, Fractal Dimension Mean
\item Area Mean, Texture SE
\item Area Mean, Smoothness SE
\item Area Mean, Symmetry SE
\item Area Mean, Fractal Dimension SE
\item Fractal Dimension Mean, Area SE
\item Fractal Dimension Mean, Radius Worst
\item Fractal Dimension Mean, Texture Worst
\item Fractal Dimension Mean, Perimeter Worst
\item Fractal Dimension Mean, Area Worst
\item Texture SE, Radius Worst
\item Texture SE, Perimeter Worst
\item Texture SE, Area Worst
\item Texture SE, Smoothness Worst
\item Texture SE, Compactness Worst
\item Texture SE, Concavity Worst
\item Texture SE, Concave Points Worst
\item Texture SE, Symmetry Worst
\item Texture SE, Fractal Dimension Worst
\item Smoothness SE, Radius Worst
\item Smoothness SE, Texture Worst
\item Smoothness SE, Perimeter Worst
\item Smoothness SE, Area Worst
\item Smoothness SE, Compactness Worst
\item Smoothness SE, Concavity Worst
\item Smoothness SE, Concave Points Worst
\item Smoothness SE, Symmetry Worst
\item Symmetry SE, Radius Worst
\item Symmetry SE, Texture Worst
\item Symmetry SE, Perimeter Worst
\item Symmetry SE, Area Worst
\item Symmetry SE, Smoothness Worst
\item Symmetry SE, Concave Points Worst
\item Fractal Dimension SE, Radius Worst
\item Fractal Dimension SE, Texture Worst
\item Fractal Dimension SE, Perimeter Worst
\item Fractal Dimension SE, Area Worst

\end{itemize}
\end{multicols}

\subsection{Matriz de Distâncias} 

As figuras \ref{fig:distorig} e \ref{fig:zscore} mostram a matriz de distâncias, utilizando a Distância Euclidiana:
\[dist_E(\nu,\upsilon) = ||\nu-\upsilon||\]


\begin{figure}[H]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{../img/distance_out}
  \caption{Antes da retirada de outliers}
  \label{fig:antes_out}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{../img/distance_clean}
  \caption{Após da retirada de outliers}
  \label{fig:apos_out}
\end{subfigure}
\caption{Matriz de distâncias}
\label{fig:distorig}
\end{figure}

A matriz de distâncias para o conjunto de registros original pode ser observada na figura \ref{fig:antes_out}. Foi realizado o processo de retirada de outliers baseado na distância média $m_i = \frac{1}{N} \Sigma^N_{j=1} d_{ij}$. Foram removidos os $P_{out} = 10\%$ registros correspondentes aos maiores valores. A matriz de distâncias do conjunto de dados resultante pode ser vista na figura \ref{fig:apos_out}.

\begin{figure}[H]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{../img/dist_zscore_out}
  \caption{Antes da retirada de outliers}
  \label{fig:zantes_out}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{../img/distance_zscore_clean}
  \caption{Após da retirada de outliers}
  \label{fig:zapos_out}
\end{subfigure}
\caption{Matriz de distâncias com z-score}
\label{fig:zscore}
\end{figure}

Como o conjunto de dados contém variáveis em unidades e escalas diferentes, o que dificulta a avaliação da matriz de distâncias pois os outliers de algumas variáveis acabam dominando. Assim, foi feita uma padronização das variáveis por meio da estimativa z-score e a matriz de distâncias resultante pode ser vista na figura \ref{fig:zscore}.

\section{Formulação do Problema}
Consiste em um problema de classificação. Deseja-se desenvolver um classificador capaz de predizer a classe do registro (maligno ou benigno).

Para solução deste problema foram aplicados os modelos de classificação a seguir e avaliados os resultados.

\begin{itemize}
\item Classificador Bayesiano Simples
\item Classificador Bayesiano Quadrático
\item Regressão Logística
\item Perceptron
\item Perceptron Múltiplas Camadas
\item SVM
\end{itemize}

\section{Metodologia}
Para os testes será utilizada a metodologia de validação cruzada de 10 ciclos. A cada iteração o dataset é dividido em subconjuntos complementares. O modelo é ajustado com o conjunto de treinamento e aplicado para predizer a saída do conjunto de teste. O resultado final é obtido como a média de cada iteração. 

Para avaliar o desempenho do classificador, os resultados preditos pelo teste são comparado com os resultados reais e contabilizada a matriz de confusão. A partir dela são calculadas as seguintes métricas:
\begin{itemize}
\item Acurácia
\item Erro global
\item Precisão 
\item Recuperação
\end{itemize}

Métodos como Perceptron, Perceptron Múltiplas Camadas e SVM apresentaram grande sensibilidade a padronização das variáveis de entrada, no entanto, os demais métodos apresentaram resultados melhores sem padronização. Portanto, a título de comparação ambos os resultados são mostrados. No caso do MLP e SVM somente foi exibido o resultado com padronização pois caso contrário apresentavam problemas de convergência. 

\section{Apresentação da Tecnologia}
Para a implementação e execução dos algoritmos foram usadas as seguintes  bibliotecas da linguagem Python:

\begin{itemize}
\item \textit{SciPy}: Ecossistema de softwares para matemática, ciência e engenharia. Contém os pacotes:
\begin{itemize}
\item \textit{NumPy}
\item \textit{matplotlib}
\item \textit{pandas}: Python Data Analysis Library
\end{itemize}
\item \textit{scikit-learn}: Machine Learning in Python
\end{itemize}


\section{Classificador Bayesiano Simples}

\subsection{Formulação matemática}
O Classificador Bayesiano baseia-se na aplicação do Teorema de Bayes com a suposição de independência entre cada par de variáveis, ou seja, que não há correlação entre as variáveis.

\[P(y|x_1 \dots x_n)  = \frac{P(y)P(x_1, \dots, x_n|y)}{P(x_1,\dots, x_n)}\]
Onde:

$y$ é a variável de saída que identifica a classe

$x = [x_1, \dots, x_n] $ é o vetor de entrada 
\\
A decisão é feita pelo valor máximo da probabilidade a posteriori, dividindo o espaço em:
\[ R_1 = {x \in R, P(C_1|x) \geq P(C_2|x)}\]
\[ R_2 = {x \in R, P(C_2|x) > P(C_1|x)}\]
\subsection{Resultados (sem padronização)}
O dataset não é linearmente separável portanto os resultados não foram tão satisfatórios, já que o classificador Bayesiano Simples gera uma fronteira de decisão linear. Aĺém disso, assume-se que a distribuição de probabilidade das características é Gaussiana, o que não se verifica para todas as características do dataset, como visto nos histogramas.

\begin{table}[H]
\centering
\caption{Matriz de confusão}
\begin{tabular}{l l l}
\hline
 & \textbf{$\hat{C}_1$ (Predita)} & \textbf{$\hat{C}_2$(Predita)}\\
\hline
$C_1$ & 18.80&2.40\\ 
$C_2$ &  1.20&34.50\\ 
\hline
\end{tabular}
\end{table}

\begin{minipage}{.5\textwidth}
\begin{itemize}
\item $ACC = 93.67 \%$
\item $ERR =  6.33 \%$
\item $AUC = 0.9266 $
\item $PRE(C1) = 0.94$
\item $REC(C1) = 0.8868$
\item $PRE(C2) = 0.9350$
\item $REC(C2) = 0.9664$
\end{itemize}
\end{minipage}%
\begin{minipage}{.5\textwidth}
\begin{figure}[H]
\centering
  \includegraphics[width=\linewidth]{../img/naive_bayes_rec.png}
  \captionof{figure}{Precisão e Recall}
  \label{fig:percep}
\end{figure}
\end{minipage}%

\subsection{Resultados (com padronização)}
\begin{table}[H]
\centering
\caption{Matriz de confusão}
\begin{tabular}{l l l}
\hline
 & \textbf{$\hat{C}_1$ (Predita)} & \textbf{$\hat{C}_2$(Predita)}\\
\hline
$C_1$ &  18.9 & 2.3 \\
$C_2$ &  1.9 & 33.8 \\
\hline
\end{tabular}
\end{table}

\begin{minipage}{.5\textwidth}
\begin{itemize}
\item $ACC =  92.62 \%$
\item $ERR =  7.38 \%$
\item $AUC =  0.9191 $
\item $PRE(C1) =  0.9087 $
\item $REC(C1) =  0.8915 $
\item $PRE(C2) =  0.9363 $
\item $REC(C2) =  0.9468 $
\end{itemize}
\end{minipage}%
\begin{minipage}{.5\textwidth}
\begin{figure}[H]
\centering
  \includegraphics[width=\linewidth]{../img/naive_bayes_rec_std.png}
  \captionof{figure}{Precisão e Recall}
  \label{fig:percep}
\end{figure}
\end{minipage}%

\section{Classificador Bayesiano Quadrático}
\subsection{Formulação Matemática}
É um classificador com uma fronteira de decisão quadrática, gerado pela densidades condicionais dos dados e utilizando a regra de Bayes.

A decisão é calculada pela função discriminante:
\[ g_i(x(t))= ln P(C_i|x(t))= ln p(x(t)|C_i)+ ln P(C_i)\]
Substituindo a expressão da distribuição normal multivariada, observa-se que a expressão da função discriminante é dominada por um termo quadrático.

\subsection{Resultados}
Verifica-se que os resultados foram melhores que os obtidos com Naive Bayes pois a superfície de decisão agora é quadrática, permitindo a classificação de modelos mais complexos. Neste caso os resultados com e sem padronização foram os mesmos. 
\begin{table}[H]
\centering
\caption{Matriz de confusão}
\begin{tabular}{l l l}
\hline
 & \textbf{$\hat{C}_1$ (Predita)} & \textbf{$\hat{C}_2$(Predita)}\\
\hline
$C_1$ & 20.00&1.20\\ 
$C_2$ &  1.20&34.50\\ 
\hline
\end{tabular}
\end{table}

\begin{minipage}{.5\textwidth}
\begin{itemize}
\item $ACC = 95.78 \%$
\item $ERR =  4.22 \%$
\item $AUC = 0.9549 $
\item $PRE(C1) = 0.9434$
\item $REC(C1) = 0.9434$
\item $PRE(C2) = 0.9664$
\item $REC(C2) = 0.9664$
\end{itemize}
\end{minipage}%
\begin{minipage}{.5\textwidth}
\begin{figure}[H]
\centering
  \includegraphics[width=\linewidth]{../img/quad_bayes_rec.png}
  \captionof{figure}{Precisão e Recall}
  \label{fig:percep}
\end{figure}
\end{minipage}%

\section{Regressão Logística}
\subsection{Formulação Matemática}
Na regressão logística, a saída do modelo é uma aproximação da probabilidade a posteriori.

A função discriminante é calculada pela função sigmoide, ou função logística ou logit:

\[g_i(x(t)|\theta_i))  = \frac{1}{1+ exp(\hat{x}(t)\theta_i)}\]
Onde:
\[\hat{x}(t) = [1,x(t)] \quad e \quad \theta_i=[\theta\textsubscript{i0},\theta\textsubscript{i1}]^T\]

\subsection{Resultados (sem padronização)}

\begin{table}[H]
\centering
\caption{Matriz de confusão}
\begin{tabular}{l l l}
\hline
 & \textbf{$\hat{C}_1$ (Predita)} & \textbf{$\hat{C}_2$(Predita)}\\
\hline
$C_1$ & 20.00&1.20\\ 
$C_2$ & 1.10&34.60\\  
\hline
\end{tabular}
\end{table}

\begin{minipage}{.5\textwidth}
\begin{itemize}
\item $ACC = 95.96 \%$
\item $ERR =  4.04 \%$
\item $AUC = 0.9563 $
\item $PRE(C1) = 0.9479$
\item $REC(C1) = 0.9434$
\item $PRE(C2) = 0.9665$
\item $REC(C2) = 0.9692$
\end{itemize}
\end{minipage}%
\begin{minipage}{.5\textwidth}
\begin{figure}[H]
\centering
  \includegraphics[width=\linewidth]{../img/log_reg_rec.png}
  \captionof{figure}{Precisão e Recall}
  \label{fig:percep}
\end{figure}
\end{minipage}%

\subsection{Resultados (com padronização)}

\begin{table}[H]
\centering
\caption{Matriz de confusão}
\begin{tabular}{l l l}
\hline
 & \textbf{$\hat{C}_1$ (Predita)} & \textbf{$\hat{C}_2$(Predita)}\\
\hline
$C_1$ &  19.9 & 1.3 \\
$C_2$ &  1.6 & 34.1 \\
\end{tabular}
\end{table}

\begin{minipage}{.5\textwidth}
\begin{itemize}
\item $ACC =  94.9 \%$
\item $ERR =  5.1 \%$
\item $AUC =  0.9469 $
\item $PRE(C1) =  0.9256 $
\item $REC(C1) =  0.9387 $
\item $PRE(C2) =  0.9633 $
\item $REC(C2) =  0.9552 $
\end{itemize}
\end{minipage}%
\begin{minipage}{.5\textwidth}
\begin{figure}[H]
\centering
  \includegraphics[width=\linewidth]{../img/log_reg_rec_std.png}
  \captionof{figure}{Precisão e Recall}
  \label{fig:percep}
\end{figure}
\end{minipage}%

\section{Perceptron}
\subsection{Formulação Matemática}
O Perceptron utiliza o modelo McCulloch-Pitts para o neurônio artificial. O processamento de cada unidade é dado por:
\[ u(t) = h(z(t)) = h \left( \theta_0 + \sum_{i=1}^n x_i(t) \theta_i \right) \]
onde:

$u(t)$: valor de ativação

$z(t)$: potencial de ativaçãos

$h$: função de ativação

$x_i(t)$: entradas do neurônio
%O Perceptron incorpora o conceito de aprendizado, ou seja, se o padrão é classificado corretamente nenhum ajuste é realizado. TODO
\\
A função custo do Perceptron é linear como mostrada na figura \ref{fig:lossfunc}, ou \textit{Hinge(0)}.

\begin{figure}[H]
\centering
  \includegraphics[width=0.5\linewidth]{../img/loss_function.png}
  \label{fig:lossfunc}
  \captionof{figure}{Funções Custo}
\end{figure}%

\subsection{Resultados (sem padronização)}

Fica evidente que o Perceptron é bastante sensível a escala das variáveis, apresentando desempenho bastante inferior ao que apresentou com padronização.
\begin{table}[H]
\centering
\caption{Matriz de confusão}
\begin{tabular}{l l l}
\hline
 & \textbf{$\hat{C}_1$ (Predita)} & \textbf{$\hat{C}_2$(Predita)}\\
\hline
$C_1$ &  16.8 & 4.4 \\
$C_2$ &  1.1 & 34.6 \\
\hline
\end{tabular}
\end{table}

\begin{minipage}{.5\textwidth}
\begin{itemize}
\item $ACC =  90.33 \%$
\item $ERR =  9.67 \%$
\item $AUC =  0.8808 $
\item $PRE(C1) =  0.9385 $
\item $REC(C1) =  0.7925 $
\item $PRE(C2) =  0.8872 $
\item $REC(C2) =  0.9692 $
\end{itemize}
\end{minipage}%
\begin{minipage}{.5\textwidth}
\begin{figure}[H]
\centering
  \includegraphics[width=\linewidth]{../img/perc_rec.png}
  \captionof{figure}{Precisão e Recall}
  \label{fig:percep}
\end{figure}
\end{minipage}%

\subsection{Resultados (com padronização)}
\begin{table}[H]
\centering
\caption{Matriz de confusão}
\begin{tabular}{l l l}
\hline
 & \textbf{$\hat{C}_1$ (Predita)} & \textbf{$\hat{C}_2$(Predita)}\\
\hline
$C_1$ & 20.40&0.80\\ 
$C_2$ & 1.40&34.30\\ 
\hline
\end{tabular}
\end{table}

\begin{minipage}{.5\textwidth}
\begin{itemize}
\item $ACC = 96.13 \%$
\item $ERR =  3.87 \% $
\item $AUC = 0.9615 $
\item $PRE(C1) = 0.9358$
\item $REC(C1) = 0.9623$
\item $PRE(C2) = 0.9772$
\item $REC(C2) = 0.9608$
\end{itemize}
\end{minipage}%
\begin{minipage}{.5\textwidth}
\begin{figure}[H]
\centering
  \includegraphics[width=\linewidth]{../img/perc_rec_std.png}
  \captionof{figure}{Precisão e Recall}
  \label{fig:percep}
\end{figure}
\end{minipage}


\section{Perceptron de Múltiplas Camadas (MLP)}
\subsection{Formulação Matemática}
Uma rede-neural MLP apresenta uma camada de entrada que não realiza processamento com a dimensão do vetor de entrada, uma ou mais camadas intermediárias que realizam processamento e uma camada de saída que, num problema de classificação é o vetor com as estimativas das variáveis indicadoras. O modelo McCulloch-Pitts é utilizado nas unidades das camadas intermediárias e de saída. Assim, o MLP é capaz de aprender uma função $f(\cdot): R^m \rightarrow R^o$, onde $m$ é dimensão do vetor de entrada e $o$ é a dimensão do vetor de saída. O Perceptron de Múltiplas Camadas é capaz de aprender modelos não lineares.

\begin{figure}[H]
\centering
  \includegraphics[width=0.43\linewidth]{../img/multilayerperceptron_network.png}
  \label{fig:percep}
  \captionof{figure}{Rede neural MLP}
\end{figure}%

%Dado um conjunto com entrada $x(t) \in \mathbf{R}^N$ e saída $  y(t) \in \{0, 1\}$

A função custo utilizada é a Entropia cruzada, que para cada elemento é dada por:
\[l(\hat{y},y,\theta) = -y \ln{\hat{y}} - (1-y) \ln{(1 - \hat{y})} + \alpha ||\theta||_2^2\]

Onde $\alpha ||\theta||_2^2$ é um termo de regularização, $\alpha > 0$ um parametro que controla a magnitude da penalidade e $\theta$ é o vetor de parâmetros.
%\[ l(\theta) = -\sum_{t=1}^N \nu(t)log(\hat{\nu}) + (1-\nu(t)) log(1-\hat{\nu}(t)) \]

A função de ativação escolhida para os neurônios das camadas intermediárias foi a Tangente hiperbólica:

\[ u(t) = \frac{1- exp(-z(t))}{1+exp(-z(t))}\]

 Método de ajuste dos parâmetros escolhido foi o Gradiente descendente estocástico:

\[\theta^{i+1} = \theta^i - \eta \nabla {l}({\theta})^{i}\]

Onde $i$ é o número da iteração e $\eta$ a taxa de aprendizado. 

Depois de calculada função custo, o ajuste é propagado para as camadas anteriores através do Backpropagation.


\textbf{Observações:}

Durante a execução do algoritmo notou-se que os resultados eram diferentes a cada execução. Isso se deve a inicialização aleatória dos parametros. Quando existem camadas intermediárias a função custo não é convexa, portanto existe mais de um mínimo local. Isso pode ser resolvido configurando manualmente a semente do gerador aleatório.

Além disso, o método é bastante sensível a escala das variáveis de entrada, tornando necessária, mais que em outros métodos, a padronização das variáveis pois caso contrário o método apresenta problemas de convergência. Foi feita uma padronização de modo a obter média zero e desvio padrão 1. A padronização é calculada no conjunto de treinamento e a mesma transformação é aplicada para o conjunto de teste.

\subsection{Resultados - (21)}

Para este teste foi utilizada uma eurística para obter o número de camadas intermediárias: 
\[N = \frac{2}{3} (m+o)\]
Sendo $m$ a dimensão da entrada e $o$ a dimensão da saída.
\begin{table}[H]
\centering
\caption{Matriz de confusão}
\begin{tabular}{l l l}
\hline
 & \textbf{$\hat{C}_1$ (Predita)} & \textbf{$\hat{C}_2$(Predita)}\\
\hline
$C_1$ & 20.20&1.00\\
$C_2$ & 0.40&35.30\\ 
\hline
\end{tabular}
\end{table}

\begin{minipage}{.5\textwidth}
\begin{itemize}
\item $ACC = 97.35 \%$
\item $ERR =  2.65$
\item $AUC = 0.9708 \%$
\item $PRE(C1) = 0.9806$
\item $REC(C1) = 0.9528$
\item $PRE(C2) = 0.9725$
\item $REC(C2) = 0.9888$
\end{itemize}
\end{minipage}%
\begin{minipage}{.5\textwidth}
\begin{figure}[H]
\centering
  \includegraphics[width=\linewidth]{../img/mlp_rec.png}
  \captionof{figure}{Precisão e Recall}
  \label{fig:percep}
\end{figure}
\end{minipage}%

\subsection{Resultados - (100)}

\begin{table}[H]
\centering
\caption{Matriz de confusão}
\begin{tabular}{l l l}
\hline
 & \textbf{$\hat{C}_1$ (Predita)} & \textbf{$\hat{C}_2$(Predita)}\\
\hline
$C_1$ & 20.40 & 0.80 \\
$C_2$ & 0.30  & 35.40\\ 
\hline
\end{tabular}
\end{table}

\begin{minipage}{.5\textwidth}
\begin{itemize}
\item $ACC = 98.07 \%$
\item $ERR =  1.93 \%$
\item $AUC = 0.9769  $
\item $PRE(C1) = 0.9855$
\item $REC(C1) = 0.9623$
\item $PRE(C2) = 0.9779$
\item $REC(C2) = 0.9916$
\end{itemize}
\end{minipage}%
\begin{minipage}{.5\textwidth}
\begin{figure}[H]
\centering
  \includegraphics[width=\linewidth]{../img/mlp_100.png}
  \captionof{figure}{Precisão e Recall}
  \label{fig:percep}
\end{figure}
\end{minipage}%

\subsection{Resultados - (10,10)}
\begin{table}[H]
\centering
\caption{Matriz de confusão}
\begin{tabular}{l l l}
\hline
 & \textbf{$\hat{C}_1$ (Predita)} & \textbf{$\hat{C}_2$(Predita)}\\
\hline
$C_1$ & 20.30 & 0.90 \\
$C_2$ & 0.30  & 35.40\\ 
\hline
\end{tabular}
\end{table}

\begin{minipage}{.5\textwidth}
\begin{itemize}
\item $ACC = 97.89 \%$
\item $ERR =  2.11 \%$
\item $AUC = 0.9746  $
\item $PRE(C1) = 0.9854$
\item $REC(C1) = 0.9575$
\item $PRE(C2) = 0.9752$
\item $REC(C2) = 0.9916$
\end{itemize}
\end{minipage}%
\begin{minipage}{.5\textwidth}
\begin{figure}[H]
\centering
  \includegraphics[width=\linewidth]{../img/mlp_10x10.png}
  \captionof{figure}{Precisão e Recall}
  \label{fig:percep}
\end{figure}
\end{minipage}%


\section{Máquinas de Vetor de Suporte (SVM)}
\subsection{Formulação Matemática}
O SVM baseia-se na representação indireta do espaço de características a partir do produto interno do espaço vetorial de funções. 

A função de núcleo utilizada foi a RBF: $\exp(-\gamma |x-x'|^2)$.

O problema de otimização para o ajuste de parâmetros pode ser escrito como:
\begin{align*}
\min_ {w, b, \zeta} \frac{1}{2} w^T w + C \sum_{i=1}^{n} \zeta_i \\
\textrm {sujeito a } & y_i (w^T \phi (x_i) + b) \geq 1 - \zeta_i,\\
& \zeta_i \geq 0, i=1, ..., n
\end{align*}

$C = \frac{N}{\alpha}$

$w:$ direção ortogonal ao hiperplano da função discriminante
\\
O método busca maximizar a margem de separação.

\subsection{Resultados}
O classificação por SVM é mais recomendada para vetores de características de dimensões maiores e poucos registros. No entanto, como o dataset em questão não possui uma quantidade muito grande de registros é válido utilizá-lo.

\begin{table}[H]
\centering
\caption{Matriz de confusão}
\begin{tabular}{l l l}
\hline
 & \textbf{$\hat{C}_1$ (Predita)} & \textbf{$\hat{C}_2$(Predita)}\\
\hline
$C_1$ & 20.40 & 0.80 \\
$C_2$ & 0.50  & 35.20\\ 
\hline
\end{tabular}
\end{table}

\begin{minipage}{.5\textwidth}
\begin{itemize}
\item $ACC = 97.72 \%$
\item $ERR =  2.28 \%$
\item $AUC = 0.9741  $
\item $PRE(C1) = 0.9761$
\item $REC(C1) = 0.9623$
\item $PRE(C2) = 0.9778$
\item $REC(C2) = 0.9860$
\end{itemize}
\end{minipage}%
\begin{minipage}{.5\textwidth}
\begin{figure}[H]
\centering
  \includegraphics[width=\linewidth]{../img/svm.png}
  \captionof{figure}{Precisão e Recall}
  \label{fig:percep}
\end{figure}
\end{minipage}%

\section{Conclusão}
A comparação entre os métodos pode ser vista na tabela \ref{tab:results}, onde (NP) ou (P) indica se foi feita padronização ou não, ressaltando que os métodos MLP e SVM apresentaram problemas de convergência sem padronização.

Conforme esperado, observou-se que com o aumento de complexidade dos modelos, os resultados melhoraram. Iniciando com o Bayesiano Simples, que supoe que as variáveis não estão correlacionadas e respeitam uma distribuição gaussiana, observaram-se resultados inferiores a todos os outros métodos. O Bayesiano Quadrático teve melhor desempenho, pois possui uma superfície de decisão quadrática, sendo portanto mais flexível. A Regressão Logística foi levemente superior, apesar de possuir uma função discriminante linear, provavelmente por utilizar o princípio da máxima verossimilhança para ajustar os parâmetros, o que é mais recomendado para problemas de classificação. 

Os modelos baseados em redes neurais apresentaram os melhores resultados. Os resultados melhoravam com maior número de neurônios na rede. No entanto a adição de mais camadas não trouxe melhoras tão significativas pelo fato de o modelo não apresentar complexidade tão grande.  A desvantagem é que conforme aumenta-se o número de neurônios e camadas o custo computacional cresce bastante. Além disso, o modelo é bastante sensível a padronização das variáveis. Como mostra a tabela o melhor desempenho obtido foi com o MLP com uma camada intermediária de 100 neurônios.

\begin{table}[H]
\centering
\caption{Resultados}
\label{tab:results}
\begin{tabular}{l l l} \hline
 & \textbf{ACC} & \textbf{AUC}\\
\hline
Baysiano Simples (NP)      & $93.67\%$          & $0.9266$          \\ 
Baysiano Simples (P)       & $92.62\%$          & $0.9191$          \\ 
Baysiano Quadrático (NP/P) & $95.78\%$          & $0.9549$          \\ 
Regressão Logistica (NP)   & $95.96\%$          & $0.9563$          \\ 
Regressão Logistica (P)    & $94.90\%$          & $0.9469$          \\ 
Perceptron (NP)            & $90.33\%$          & $0.8808$          \\ 
Perceptron (P)             & $96.13\%$          & $0.9615$          \\ 
MPL (21) (P)               & $97.35\%$          & $0.9708$          \\ 
MPL(100) (P)               & \textbf{$98.07\%$} & \textbf{$0.9769$} \\ 
MPL(10,10) (P)             & $97.89\%$          & $0.9746$          \\ 
SVM (P)                    & $97.72\%$          & $0.9741$          \\ 
\hline
\end{tabular}
\end{table}


\section{Apêndice}
\begin{table}[H]
\centering
\caption{Estatísticas básicas (todas as variáveis)}
\label{my-label}
\begin{tabular}{llllllll} \hline
         & Máximo  & Mínimo   & Média    & Desvio Padrão & P 25         & P 50         & P 75         \\ \hline
$x_{1}$  & 28.11   & 6.981    & 14.13    & 3.524         & 11.7         & 13.37        & 15.78        \\
$x_{2}$  & 2.873   & 0.1115   & 0.4052   & 0.2773        & 0.2324       & 0.3242       & 0.4789       \\
$x_{3}$  & 36.04   & 7.93     & 16.27    & 4.833         & 13.01        & 14.97        & 18.79        \\
$x_{4}$  & 39.28   & 9.71     & 19.29    & 4.301         & 16.17        & 18.84        & 21.8         \\
$x_{5}$  & 4.885   & 0.3602   & 1.217    & 0.5516        & 0.8339       & 1.108        & 1.474        \\
$x_{6}$  & 49.54   & 12.02    & 25.68    & 6.146         & 21.08        & 25.41        & 29.72        \\
$x_{7}$  & 188.5   & 43.79    & 91.97    & 24.3          & 75.17        & 86.24        & 104.1        \\
$x_{8}$  & 21.98   & 0.757    & 2.866    & 2.022         & 1.606        & 2.287        & 3.357        \\
$x_{9}$  & 251.2   & 50.41    & 107.3    & 33.6          & 84.11        & 97.66        & 125.4        \\
$x_{10}$ & 2501    & 143.5    & 654.9    & 351.9         & 420.3        & 551.1        & 782.7        \\
$x_{11}$ & 542.2   & 6.802    & 40.34    & 45.49         & 17.85        & 24.53        & 45.19        \\
$x_{12}$ & 4254    & 185.2    & 880.6    & 569.4         & 515.3        & 686.5        & 1084         \\
$x_{13}$ & 0.1634  & 0.05263  & 0.09636  & 0.01406       & 0.08637      & 0.09587      & 0.1053       \\
$x_{14}$ & 0.03113 & 0.001713 & 0.007041 & 0.003003      & 0.005169     & 0.00638      & 0.008146     \\
$x_{15}$ & 0.2226  & 0.07117  & 0.1324   & 0.02283       & 0.1166       & 0.1313       & 0.146        \\
$x_{16}$ & 0.3454  & 0.01938  & 0.1043   & 0.05281       & 0.06492      & 0.09263      & 0.1304       \\
$x_{17}$ & 0.1354  & 0.002252 & 0.02548  & 0.01791       & 0.01308      & 0.02045      & 0.03245      \\
$x_{18}$ & 1.058   & 0.02729  & 0.2543   & 0.1573        & 0.1472       & 0.2119       & 0.3391       \\
$x_{19}$ & 0.4268  & 0        & 0.0888   & 0.07972       & 0.02956      & 0.06154      & 0.1307       \\
$x_{20}$ & 0.396   & 0        & 0.03189  & 0.03019       & 0.01509      & 0.02589      & 0.04205      \\
$x_{21}$ & 1.252   & 0        & 0.2722   & 0.2086        & 0.1145       & 0.2267       & 0.3829       \\
$x_{22}$ & 0.2012  & 0        & 0.04892  & 0.0388        & 0.02031      & 0.0335       & 0.074        \\
$x_{23}$ & 0.05279 & 0        & 0.0118   & 0.00617       & 0.007638     & 0.01093      & 0.01471      \\
$x_{24}$ & 0.291   & 0        & 0.1146   & 0.06573       & 0.06493      & 0.09993      & 0.1614       \\
$x_{25}$ & 0.304   & 0.106    & 0.1812   & 0.02741       & 0.1619       & 0.1792       & 0.1957       \\
$x_{26}$ & 0.07895 & 0.007882 & 0.02054  & 0.008266      & 0.01516      & 0.01873      & 0.02348      \\
$x_{27}$ & 0.6638  & 0.1565   & 0.2901   & 0.06187       & 0.2504       & 0.2822       & 0.3179       \\
$x_{28}$ & 0.09744 & 0.04996  & 0.0628   & 0.00706       & 0.0577       & 0.06154      & 0.06612      \\
$x_{29}$ & 0.02984 & 0.000895 & 0.003795 & 0.002646      & 0.002248     & 0.003187     & 0.004558     \\
$x_{30}$ & 0.2075  & 0.05504  & 0.08395  & 0.01806       & 0.07146      & 0.08004      & 0.09208      \\ \hline
\end{tabular}
\end{table}

\end{document}
